{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fll0QkbYaKZ",
        "outputId": "9c1c9ac8-1043-4a11-bede-a72a754000f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Installing necessary libraries\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is amazing! It helps in tokenization, stemming, and lemmatization. Let's test it with tweets like #AI ðŸ˜Š\"\n"
      ],
      "metadata": {
        "id": "nZUFJ624Y6i0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Various Tokenization Techniques\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(wt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkAVr_I2Zsai",
        "outputId": "dbecb8c7-6df7-4029-bc12-8f11a409bc01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "['NLTK', 'is', 'amazing!', 'It', 'helps', 'in', 'tokenization,', 'stemming,', 'and', 'lemmatization.', \"Let's\", 'test', 'it', 'with', 'tweets', 'like', '#AI', 'ðŸ˜Š']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "print(\"\\nPunctuation-based Tokenization:\")\n",
        "print(wordpunct_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SXuz-43Zw0j",
        "outputId": "19470f92-2255-4171-c7f0-74850e30d6c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation-based Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', 'It', 'helps', 'in', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.', 'Let', \"'\", 's', 'test', 'it', 'with', 'tweets', 'like', '#', 'AI', 'ðŸ˜Š']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tb = TreebankWordTokenizer()\n",
        "print(\"\\nTreebank Tokenization:\")\n",
        "print(tb.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldHJUPf1Z1cM",
        "outputId": "a03ff5b4-e6ab-4d76-85a2-b07fa9499f50"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', 'It', 'helps', 'in', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization.', 'Let', \"'s\", 'test', 'it', 'with', 'tweets', 'like', '#', 'AI', 'ðŸ˜Š']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihRMtlJyaTlx",
        "outputId": "22abf525-56c8-42db-d79b-6c359f1dba5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', 'It', 'helps', 'in', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.', \"Let's\", 'test', 'it', 'with', 'tweets', 'like', '#AI', 'ðŸ˜Š']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Various Stemming Techniques\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairness\"]\n",
        "\n",
        "print(\"\\nPorter Stemmer:\")\n",
        "for word in words:\n",
        "    print(word, \"â†’\", ps.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmWtYHulaeIq",
        "outputId": "fbe5c10f-a6de-4f6e-c830-d641b85c6d3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Porter Stemmer:\n",
            "running â†’ run\n",
            "runs â†’ run\n",
            "runner â†’ runner\n",
            "easily â†’ easili\n",
            "fairness â†’ fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "\n",
        "print(\"\\nSnowball Stemmer:\")\n",
        "for word in words:\n",
        "    print(word, \"â†’\", ss.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOpbffCgaig9",
        "outputId": "9b4a012a-1464-410c-fc7a-a208a3340ec7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Snowball Stemmer:\n",
            "running â†’ run\n",
            "runs â†’ run\n",
            "runner â†’ runner\n",
            "easily â†’ easili\n",
            "fairness â†’ fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Various Lemmatizing Techniques\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatization_words = [\"running\", \"better\", \"cars\", \"studies\"]\n",
        "\n",
        "print(\"\\nLemmatization:\")\n",
        "for word in lemmatization_words:\n",
        "    print(word, \"â†’\", lemmatizer.lemmatize(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFPhcgi9anTS",
        "outputId": "28a3b276-7c74-4b19-921e-4a7d170c71ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            "running â†’ running\n",
            "better â†’ better\n",
            "cars â†’ car\n",
            "studies â†’ study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbjYqFJ2at2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}