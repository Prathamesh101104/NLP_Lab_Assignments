{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fll0QkbYaKZ",
        "outputId": "e2a0fcc1-2ebb-4c86-eef7-de0da3ed59af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Error loading pos_tag: Package 'pos_tag' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#Installing necessary libraries\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('pos_tag')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is amazing! It helps in tokenization, stemming, and lemmatization. Let's test it with tweets like #AI ðŸ˜Š\"\n"
      ],
      "metadata": {
        "id": "nZUFJ624Y6i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Various Tokenization Techniques\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(wt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkAVr_I2Zsai",
        "outputId": "3d2cac11-4332-409e-c177-f971dca7a768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "['NLTK', 'is', 'amazing!', 'It', 'helps', 'in', 'tokenization,', 'stemming,', 'and', 'lemmatization.', \"Let's\", 'test', 'it', 'with', 'tweets', 'like', '#AI', 'ðŸ˜Š']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "print(\"\\nPunctuation-based Tokenization:\")\n",
        "print(wordpunct_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SXuz-43Zw0j",
        "outputId": "e27ec671-1d9b-451b-ea7b-05c41e604169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation-based Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', 'It', 'helps', 'in', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.', 'Let', \"'\", 's', 'test', 'it', 'with', 'tweets', 'like', '#', 'AI', 'ðŸ˜Š']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tb = TreebankWordTokenizer()\n",
        "print(\"\\nTreebank Tokenization:\")\n",
        "print(tb.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldHJUPf1Z1cM",
        "outputId": "dee09d8b-c2bc-47fe-951c-466c72bb1b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', 'It', 'helps', 'in', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization.', 'Let', \"'s\", 'test', 'it', 'with', 'tweets', 'like', '#', 'AI', 'ðŸ˜Š']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihRMtlJyaTlx",
        "outputId": "958f9eda-59f2-4766-c196-99e4be418536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', 'It', 'helps', 'in', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.', \"Let's\", 'test', 'it', 'with', 'tweets', 'like', '#AI', 'ðŸ˜Š']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('machine', 'learning'), ('artificial', 'intelligence')], separator='_')\n",
        "sample_text = \"machine learning and artificial intelligence are core AI fields\"\n",
        "\n",
        "print(\"\\nMWE Tokenization:\")\n",
        "print(mwe.tokenize(sample_text.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do5Z3L_LePz9",
        "outputId": "34c8a262-63be-4b05-c205-473a857700ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MWE Tokenization:\n",
            "['machine_learning', 'and', 'artificial_intelligence', 'are', 'core', 'AI', 'fields']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Various Stemming Techniques\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairness\"]\n",
        "\n",
        "print(\"\\nPorter Stemmer:\")\n",
        "for word in words:\n",
        "    print(word, \"â†’\", ps.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmWtYHulaeIq",
        "outputId": "71678574-a2be-45e8-8c00-25a709d4fafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Porter Stemmer:\n",
            "running â†’ run\n",
            "runs â†’ run\n",
            "runner â†’ runner\n",
            "easily â†’ easili\n",
            "fairness â†’ fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "\n",
        "print(\"\\nSnowball Stemmer:\")\n",
        "for word in words:\n",
        "    print(word, \"â†’\", ss.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOpbffCgaig9",
        "outputId": "3045ab91-c736-4512-b085-fdde64441e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Snowball Stemmer:\n",
            "running â†’ run\n",
            "runs â†’ run\n",
            "runner â†’ runner\n",
            "easily â†’ easili\n",
            "fairness â†’ fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatizing Technique\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "# REQUIRED for newer NLTK versions\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatization_words = [\"Swimming\", \"worse\", \"cars\", \"studies\"]\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "print(\"\\nLemmatization:\")\n",
        "for word, tag in pos_tag(lemmatization_words):\n",
        "    print(word, \"â†’\", lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iouWIJCziwwO",
        "outputId": "123665f3-b8ac-4a07-d18a-5e1096343327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            "Swimming â†’ Swimming\n",
            "worse â†’ bad\n",
            "cars â†’ car\n",
            "studies â†’ study\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "goqmYxBVlWcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}